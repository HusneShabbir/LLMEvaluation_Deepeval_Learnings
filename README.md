# AI Evaluation with DeepEval

This repository documents my hands-on learning, experiments, and evaluations using [DeepEval](https://github.com/confident-ai/deepeval) â€” an open-source framework for testing and evaluating large language model (LLM) applications.

It includes:
- Evaluation scripts and test cases for various LLM prompts and chains
- Exploration of metrics like factual accuracy, coherence, toxicity, and custom metrics
- Real-world examples of integrating DeepEval into LLM pipelines
- Notes and insights gained through experimentation

Feel free to explore, clone, or contribute if you're also exploring AI evaluation!

